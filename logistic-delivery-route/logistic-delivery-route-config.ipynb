{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52d39e70-0b47-4c3b-a666-7e8db4f104e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delivery Route Optimization, using Spark to process large volumes of location and traffic data in real-time, optimizing delivery routes to reduce costs and time.\n",
    "\n",
    "**Databricks Environment Setup**\n",
    "\n",
    "Create a Databricks cluster with Apache Spark support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe350a9-d4a2-4cf0-a8c3-158668f5c433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting networkx\n  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\nInstalling collected packages: networkx\nSuccessfully installed networkx-3.2.1\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: geopy in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages (2.4.1)\nRequirement already satisfied: geographiclib<3,>=1.52 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages (from geopy) (2.0)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting googlemaps\n  Downloading googlemaps-4.10.0.tar.gz (33 kB)\nRequirement already satisfied: requests<3.0,>=2.20.0 in /databricks/python3/lib/python3.9/site-packages (from googlemaps) (2.27.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0,>=2.20.0->googlemaps) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0,>=2.20.0->googlemaps) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0,>=2.20.0->googlemaps) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0,>=2.20.0->googlemaps) (1.26.9)\nBuilding wheels for collected packages: googlemaps\n  Building wheel for googlemaps (setup.py): started\n  Building wheel for googlemaps (setup.py): finished with status 'done'\n  Created wheel for googlemaps: filename=googlemaps-4.10.0-py3-none-any.whl size=40716 sha256=2fd5729e067626e5b63129c536824192d54d011089b6dece17e24dad4854b4dd\n  Stored in directory: /root/.cache/pip/wheels/d9/5f/46/54a2bdb4bcb07d3faba4463d2884865705914cc72a7b8bb5f0\nSuccessfully built googlemaps\nInstalling collected packages: googlemaps\nSuccessfully installed googlemaps-4.10.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "#Required libraries\n",
    "%pip install networkx\n",
    "%pip install geopy\n",
    "%pip install googlemaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f12006-82d5-4df9-a64e-4ec6b7223741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check installments\n",
    "import networkx as nx\n",
    "import geopy\n",
    "from googlemaps import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a186a0-f4fd-45bb-9a20-2def7064ef59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: True"
     ]
    }
   ],
   "source": [
    "#Create folders\n",
    "dbutils.fs.mkdirs(\"/mnt/lhdw/landingzone/route/toprocess\")\n",
    "dbutils.fs.mkdirs(\"/mnt/lhdw/landingzone/route/processed\")\n",
    "dbutils.fs.mkdirs(\"/mnt/lhdw/bronze\")\n",
    "dbutils.fs.mkdirs(\"/mnt/lhdw/silver\")\n",
    "dbutils.fs.mkdirs(\"/mnt/lhdw/gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e445aa1-aa0f-4887-97ed-a318519bc691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed/\nto-process/\ntoprocess/\n"
     ]
    }
   ],
   "source": [
    "# List contents of the specified directory\n",
    "folders = dbutils.fs.ls(\"/mnt/lhdw/landingzone/route/\")\n",
    "for folder in folders:\n",
    "    print(folder.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83afe372-3809-4837-ab2c-e805295e60cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import files \n",
    "\n",
    "Basic data to map locations, orders and logistics constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45bf3cc7-bd21-43c7-ade5-fe8be4266e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo orders.csv baixado e salvo em: /mnt/lhdw/landingzone/route/toprocess/orders.csv\nArquivo locations.csv baixado e salvo em: /mnt/lhdw/landingzone/route/toprocess/locations.csv\nArquivo constraints.csv baixado e salvo em: /mnt/lhdw/landingzone/route/toprocess/constraints.csv\n"
     ]
    }
   ],
   "source": [
    "# To load 1 file\n",
    "# import urllib.request\n",
    "\n",
    "# # GitHub URL file\n",
    "# url = 'https://github.com/VALQUIRIAFABRO/databricks/blob/main/logistic-delivery-route/orders.csv'\n",
    "\n",
    "# # Path for temporary cluster instance\n",
    "# temp_path = '/tmp/orders.csv'\n",
    "\n",
    "# # insert file into temp path\n",
    "# urllib.request.urlretrieve(url, temp_path)\n",
    "\n",
    "# # destination path to DBFS\n",
    "# dbfs_path = '/mnt/lhdw/landingzone/route/toprocess/orders.csv'\n",
    "\n",
    "# # Move file to DBFS\n",
    "# dbutils.fs.cp(f'file:{temp_path}', f'dbfs:{dbfs_path}')\n",
    "\n",
    "# print(f\"File downloaded and saved in: {dbfs_path}\")\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# GitHub URL file\n",
    "urls = [\n",
    "    'https://github.com/VALQUIRIAFABRO/databricks/blob/main/logistic-delivery-route/orders.csv',\n",
    "    'https://github.com/VALQUIRIAFABRO/databricks/blob/main/logistic-delivery-route/locations.csv',\n",
    "    'https://github.com/VALQUIRIAFABRO/databricks/blob/main/logistic-delivery-route/constraints.csv'\n",
    "]\n",
    "\n",
    "# Caminho da instância temporária\n",
    "temp_base_path = '/tmp/'\n",
    "\n",
    "# Caminho base no DBFS\n",
    "dbfs_base_path = '/mnt/lhdw/landingzone/route/toprocess/'\n",
    "\n",
    "for url in urls:\n",
    "    # Nome do arquivo (extraído da URL)\n",
    "    file_name = url.split('/')[-1]\n",
    "    \n",
    "    # Caminhos temporário e destino\n",
    "    temp_path = f'{temp_base_path}{file_name}'\n",
    "    dbfs_path = f'{dbfs_base_path}{file_name}'\n",
    "    \n",
    "    # Fazer o download do arquivo\n",
    "    urllib.request.urlretrieve(url, temp_path)\n",
    "    \n",
    "    # Mover o arquivo para o DBFS\n",
    "    dbutils.fs.cp(f'file:{temp_path}', f'dbfs:{dbfs_path}')\n",
    "    \n",
    "    print(f\"Arquivo {file_name} baixado e salvo em: {dbfs_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fbd2fb-2748-4853-b00a-709f2f8a9013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [FileInfo(path='dbfs:/mnt/lhdw/landingzone/route/toprocess/constraints.csv', name='constraints.csv', size=213769, modificationTime=1742337999000),\n FileInfo(path='dbfs:/mnt/lhdw/landingzone/route/toprocess/locations.csv', name='locations.csv', size=214254, modificationTime=1742337998000),\n FileInfo(path='dbfs:/mnt/lhdw/landingzone/route/toprocess/orders.csv', name='orders.csv', size=213741, modificationTime=1742337998000)]"
     ]
    }
   ],
   "source": [
    "# you can list the files and directories within the specified path in DBFS Databricks Utilities (dbutils)\n",
    "dbutils.fs.ls(\"/mnt/lhdw/landingzone/route/toprocess/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2712a27-d821-4d68-99cf-cec743edc734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Medalion Architecture - Bronze Layer\n",
    "\n",
    "This code is configuring and creating a SparkSession optimized for loading data into a \"Bronze\" table or layer, which is typically the first stage of the data pipeline in a Delta Lake or data lakehouse architecture.\n",
    "\n",
    "**Code Explanation**\n",
    "\n",
    "1. Initializes the process of building a SparkSession. \n",
    "\n",
    "`SparkSession.builder`\n",
    "\n",
    "\n",
    "2. Sets the name of the application to \"Load Data Bronze\". This name helps identify the application when running it, for instance, in the Spark UI.\n",
    "\n",
    "`.appName(\"Load Data Bronze\")`\n",
    "\n",
    "3. Sets the default number of partitions for shuffling operations. Shuffling happens during operations like join or groupBy. Setting it to 200 means the shuffle output will be split into 200 partitions, which can help optimize performance based on your data size. In this project we are using files with small size, so consider increase the size according to the amount of data you are ingesting.\n",
    "\n",
    "number of partitions = number of CPU cores * 2 or 3\n",
    "\n",
    "`.config(\"spark.sql.shuffle.partitions\", \"20\")`\n",
    "\n",
    "4. Sets the maximum size of a single partition when reading files. Here it's set to 128MB, which ensures that large files are divided into manageable chunks for processing and avoid creating too many small files, which can degrade read and write performance. This helps ensure that Spark uses all available cores.\n",
    "\n",
    "`.config(\"spark.sql.files.maxPartitionBytes\", \"128MB\")`\n",
    "\n",
    "5. Specifies the compression codec for saving data in Parquet format. \"Snappy\" is a lightweight, fast compression format often used for columnar storage like Parquet.\n",
    "\n",
    "`.config(\"spark.sql.parquet.compression.codec\", \"snappy\")`\n",
    "\n",
    "6. Enables Adaptive Query Execution (AQE), a feature in Spark that dynamically and automatically optimizes execution plan at runtime based on data size being processed. It improves performance for queries involving skewed data or large shuffle stages.\n",
    "\n",
    "`.config(\"spark.sql.adaptive.enabled\", \"true\")`\n",
    "\n",
    "7. Finalizes and creates the SparkSession object. If a SparkSession already exists, it reuses the existing one instead of creating a new instance.\n",
    "\n",
    "`.getOrCreate()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2e2745-d7b9-4deb-96ad-f1a7532ade77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Start SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load Data Bronze\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lz_path_in = \"/mnt/lhdw/landingzone/route/toprocess\"\n",
    "lz_path_out = \"/mnt/lhdw/landingzone/route/processed\"\n",
    "bronze_path = \"/mnt/lhdw/bronze/route\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2139ff22-143e-4047-abf3-6c949fc83d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------------+-------------+\n|order_id|customer_id|delivery_window|     filename|\n+--------+-----------+---------------+-------------+\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n|    null|       null|           null|locations.csv|\n+--------+-----------+---------------+-------------+\nonly showing top 20 rows\n\n+----+----+--------+---------+-------------+\n|  id|name|latitude|longitude|     filename|\n+----+----+--------+---------+-------------+\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n|null|null|    null|     null|locations.csv|\n+----+----+--------+---------+-------------+\nonly showing top 20 rows\n\n+--------------------+-----+-------------+\n|          constraint|limit|     filename|\n+--------------------+-----+-------------+\n|               <html| null|locations.csv|\n|           lang=\"en\"| null|locations.csv|\n|  data-color-mode...| null|locations.csv|\n|  data-a11y-anima...| null|locations.csv|\n|                   >| null|locations.csv|\n|              <head>| null|locations.csv|\n|    <meta charset...| null|locations.csv|\n|  <link rel=\"dns-...| null|locations.csv|\n|  <link rel=\"dns-...| null|locations.csv|\n|  <link rel=\"dns-...| null|locations.csv|\n|  <link rel=\"dns-...| null|locations.csv|\n|  <link rel=\"prec...| null|locations.csv|\n|  <link rel=\"prec...| null|locations.csv|\n|  <link crossorig...| null|locations.csv|\n|    <link crossor...| null|locations.csv|\n|    <link crossor...| null|locations.csv|\n|    <link crossor...| null|locations.csv|\n|    <link crossor...| null|locations.csv|\n|  <link crossorig...| null|locations.csv|\n|<link crossorigin...| null|locations.csv|\n+--------------------+-----+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Define Raw data Schema \n",
    "# Define schemas for each file\n",
    "schema_mapping = {\n",
    "    \"orders.csv\": StructType([\n",
    "        StructField(\"order_id\", IntegerType(), True),\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"delivery_window\", DateType(), True)\n",
    "    ]),\n",
    "    \"locations.csv\": StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"latitude\", IntegerType(), True),\n",
    "        StructField(\"longitude\", IntegerType(), True)\n",
    "    ]),\n",
    "    \"constraints.csv\": StructType([\n",
    "        StructField(\"constraint\", StringType(), True),\n",
    "        StructField(\"limit\", IntegerType(), True)\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Function to get schema dynamically based on filename\n",
    "def get_schema(file_name):\n",
    "    return schema_mapping.get(file_name, None)\n",
    "\n",
    "# Looping through files dynamically\n",
    "file_list = [\"orders.csv\", \"locations.csv\", \"constraints.csv\"] \n",
    "\n",
    "for file_name in file_list:\n",
    "    schema = get_schema(file_name)\n",
    "    if schema:       \n",
    "        df_vendas = spark.read.option(\"header\", \"true\").schema(schema).csv(lz_path_in) \\\n",
    "                              .withColumn(\"filename\", regexp_extract(input_file_name(), \"([^/]+)$\", 0))\n",
    "        distinct_filenames = df_vendas.select(\"filename\").distinct()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3203ef5-b997-43c7-a567-0b1f716a1d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>filename</th></tr></thead><tbody><tr><td>locations.csv</td></tr><tr><td>constraints.csv</td></tr><tr><td>orders.csv</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "locations.csv"
        ],
        [
         "constraints.csv"
        ],
        [
         "orders.csv"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "filename",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(distinct_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e53414-3854-4105-ae11-ff6690eaddac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Mapping:\nFile: orders.csv, Schema: StructType([StructField('order_id', IntegerType(), True), StructField('customer_id', IntegerType(), True), StructField('delivery_window', DateType(), True)])\nFile: locations.csv, Schema: StructType([StructField('id', IntegerType(), True), StructField('name', StringType(), True), StructField('latitude', IntegerType(), True), StructField('longitude', IntegerType(), True)])\nFile: constraints.csv, Schema: StructType([StructField('constraint', StringType(), True), StructField('limit', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Print the schema mapping\n",
    "print(\"Schema Mapping:\")\n",
    "for file_name, schema in schema_mapping.items():\n",
    "    print(f\"File: {file_name}, Schema: {schema}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "logistic-delivery-route-config",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
